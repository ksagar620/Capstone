{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ce530c",
   "metadata": {},
   "source": [
    "# Task 2.2: Data Analysis using Big Data tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2714d",
   "metadata": {},
   "source": [
    "### Loading data into PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72721877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "import pyspark\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d951a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"capstone_checkpoint_two\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1674be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Code: string (nullable = true)\n",
      " |-- State_Code: string (nullable = true)\n",
      " |-- Zip_Code: integer (nullable = true)\n",
      " |-- City_Name: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('hdfs://localhost:54310/user/hduser/User_product_purchase_details_p2.csv', inferSchema=\"true\", header=\"true\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bdb001c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS capstone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258a7239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|    capstone|\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af664a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85d122a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+------+-----+----------+---------+----------+--------+---------------+----------+\n",
      "|User_ID|Product_ID|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|Gender|  Age|Occupation|City_Code|State_Code|Zip_Code|      City_Name|     State|\n",
      "+-------+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+------+-----+----------+---------+----------+--------+---------------+----------+\n",
      "|1000001| P00069042|            A|                         2|             0|                 3|              null|              null|    8370|     F| 0-17|        10|     C259|       S16|   42420|      Henderson|  Kentucky|\n",
      "|1000001| P00248942|            A|                         2|             0|                 1|                 6|                14|   15200|     F| 0-17|        10|     C259|       S16|   42420|      Henderson|  Kentucky|\n",
      "|1000001| P00087842|            A|                         2|             0|                12|              null|              null|    1422|     F| 0-17|        10|     C259|       S16|   42420|      Henderson|  Kentucky|\n",
      "|1000001| P00085442|            A|                         2|             0|                12|                14|              null|    1057|     F| 0-17|        10|     C259|       S16|   42420|      Henderson|  Kentucky|\n",
      "|1000002| P00285442|            C|                        4+|             0|                 8|              null|              null|    7969|     M|  55+|        16|     C259|       S16|   42420|      Henderson|  Kentucky|\n",
      "|1000003| P00193542|            A|                         3|             0|                 1|                 2|              null|   15227|     M|26-35|        15|      C64|       S04|   90036|    Los Angeles|California|\n",
      "|1000004| P00184942|            B|                         2|             1|                 1|                 8|                17|   19215|     M|46-50|         7|     C150|       S09|   33311|Fort Lauderdale|   Florida|\n",
      "|1000004| P00346142|            B|                         2|             1|                 1|                15|              null|   15854|     M|46-50|         7|     C150|       S09|   33311|Fort Lauderdale|   Florida|\n",
      "|1000004|  P0097242|            B|                         2|             1|                 1|                16|              null|   15686|     M|46-50|         7|     C150|       S09|   33311|Fort Lauderdale|   Florida|\n",
      "|1000005| P00274942|            A|                         1|             1|                 8|              null|              null|    7871|     M|26-35|        20|     C150|       S09|   33311|Fort Lauderdale|   Florida|\n",
      "|1000005| P00251242|            A|                         1|             1|                 5|                11|              null|    5254|     M|26-35|        20|     C150|       S09|   33311|Fort Lauderdale|   Florida|\n",
      "|1000005| P00014542|            A|                         1|             1|                 8|              null|              null|    3957|     M|26-35|        20|     C150|       S09|   33311|Fort Lauderdale|   Florida|\n",
      "|1000005| P00031342|            A|                         1|             1|                 8|              null|              null|    6073|     M|26-35|        20|     C150|       S09|   33311|Fort Lauderdale|   Florida|\n",
      "|1000005| P00145042|            A|                         1|             1|                 1|                 2|                 5|   15665|     M|26-35|      null|     null|       S09|   33311|           null|   Florida|\n",
      "|1000006| P00231342|            A|                         1|             0|                 5|                 8|                14|    5378|     F|51-55|      null|     null|       S04|   90032|           null|California|\n",
      "|1000006| P00190242|            A|                         1|             0|                 4|                 5|              null|    2079|     F| null|      null|     null|       S04|    null|           null|California|\n",
      "|1000006|  P0096642|            A|                         1|             0|                 2|                 3|                 4|   13055|     F| null|      null|     null|       S04|    null|           null|California|\n",
      "|1000006| P00058442|            A|                         1|             0|                 5|                14|              null|    8851|     F| null|      null|     null|       S04|    null|           null|California|\n",
      "|1000007| P00036842|            B|                         1|             1|                 1|                14|                16|   11788|     M| null|      null|     null|       S04|    null|           null|California|\n",
      "|1000008| P00249542|            C|                        4+|             1|                 1|                 5|                15|   19614|     M| null|      null|     null|      null|    null|           null|      null|\n",
      "+-------+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+------+-----+----------+---------+----------+--------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.write.mode(\"overwrite\").saveAsTable(\"capstone.purchase\")\n",
    "table=spark.sql('select * from capstone.purchase')\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bbc2da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use capstone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eef8c304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|capstone| purchase|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "097bd059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Code: string (nullable = true)\n",
      " |-- State_Code: string (nullable = true)\n",
      " |-- Zip_Code: integer (nullable = true)\n",
      " |-- City_Name: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "127d6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.sql('select * from capstone.purchase')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1f19e",
   "metadata": {},
   "source": [
    "DATA PROFILING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f525b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+------+------+----------+---------+----------+--------+---------+------+\n",
      "|User_ID|Product_ID|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|Gender|   Age|Occupation|City_Code|State_Code|Zip_Code|City_Name| State|\n",
      "+-------+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+------+------+----------+---------+----------+--------+---------+------+\n",
      "|      0|         0|            0|                         0|             0|                 0|            173638|            383247|       0|547782|550053|    550055|   550055|    550049|  550053|   550055|550049|\n",
      "+-------+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+------+------+----------+---------+----------+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the no. of null for each column \n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35bffd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+-----------------------+-----------------+\n",
      "|avg(Product_Category_1)|avg(Product_Category_2)|avg(Product_Category_3)|    avg(Purchase)|\n",
      "+-----------------------+-----------------------+-----------------------+-----------------+\n",
      "|      5.404270017525106|      9.842329251122386|     12.668243206790512|9263.968712959126|\n",
      "+-----------------------+-----------------------+-----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the mean of each column\n",
    "df.select(mean ('Product_Category_1'), mean ('Product_Category_2'), mean ('Product_Category_3'), mean ('Purchase')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e63ae33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|max(Product_Category_1)|\n",
      "+-----------------------+\n",
      "|                     20|\n",
      "+-----------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|max(Product_Category_2)|\n",
      "+-----------------------+\n",
      "|                     18|\n",
      "+-----------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|max(Product_Category_3)|\n",
      "+-----------------------+\n",
      "|                     18|\n",
      "+-----------------------+\n",
      "\n",
      "+-------------+\n",
      "|max(Purchase)|\n",
      "+-------------+\n",
      "|        23961|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the max and min values of each column\n",
    "df.agg({'Product_Category_1': 'max'}).show()\n",
    "df.agg({'Product_Category_2': 'max'}).show()\n",
    "df.agg({'Product_Category_3': 'max'}).show()\n",
    "df.agg({'Purchase': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "576a59ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|min(Product_Category_1)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "+-----------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|min(Product_Category_2)|\n",
      "+-----------------------+\n",
      "|                      2|\n",
      "+-----------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|min(Product_Category_3)|\n",
      "+-----------------------+\n",
      "|                      3|\n",
      "+-----------------------+\n",
      "\n",
      "+-------------+\n",
      "|min(Purchase)|\n",
      "+-------------+\n",
      "|           12|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Product_Category_1': 'min'}).show()\n",
    "df.agg({'Product_Category_2': 'min'}).show()\n",
    "df.agg({'Product_Category_3': 'min'}).show()\n",
    "df.agg({'Purchase': 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a1aa899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+------+----+-----------------+---------+----------+-----------------+---------------+----------+\n",
      "|summary|           User_ID|Product_ID|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|Gender| Age|       Occupation|City_Code|State_Code|         Zip_Code|      City_Name|     State|\n",
      "+-------+------------------+----------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+------+----+-----------------+---------+----------+-----------------+---------------+----------+\n",
      "|  count|            550068|    550068|       550068|                    550068|             550068|            550068|            376430|            166821|           550068|  2286|  15|               13|       13|        19|               15|             13|        19|\n",
      "|   mean|1003028.8424013031|      null|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 9.842329251122386|12.668243206790512|9263.968712959126|  null|null|13.23076923076923|     null|      null|          43910.4|           null|      null|\n",
      "| stddev| 1727.591585530871|      null|         null|         0.989086680757309| 0.4917701263173259| 3.936211369201324| 5.086589648693526| 4.125337631575267|5023.065393820593|  null|null|5.418392230078435|     null|      null|19206.87686667014|           null|      null|\n",
      "|    min|           1000001| P00000142|            A|                         0|                  0|                 1|                 2|                 3|               12|     F|0-17|                7|     C150|       S04|            33311|Fort Lauderdale|California|\n",
      "|    max|           1006040|  P0099942|            C|                        4+|                  1|                20|                18|                18|            23961|     M| 55+|               20|      C64|       S16|            90036|    Los Angeles|  Kentucky|\n",
      "+-------+------------------+----------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+------+----+-----------------+---------+----------+-----------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "203ef904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f5b33df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT Product_ID)|\n",
      "+--------------------------+\n",
      "|                      3631|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"Product_ID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e50d673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT User_ID)|\n",
      "+-----------------------+\n",
      "|                   5891|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"User_ID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a027dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT City_Name)|\n",
      "+-------------------------+\n",
      "|                        3|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"City_Name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15a95a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT State)|\n",
      "+---------------------+\n",
      "|                    3|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(\"State\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad5cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6500b428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+------+---+----------+---------+----------+--------+---------+-----+\n",
      "|User_ID|Product_ID|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|Gender|Age|Occupation|City_Code|State_Code|Zip_Code|City_Name|State|\n",
      "+-------+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+------+---+----------+---------+----------+--------+---------+-----+\n",
      "|      0|         0|            0|                         0|             0|                 0|                 0|                 0|       0|     0|  0|         0|        0|         0|       0|        0|    0|\n",
      "+-------+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+------+---+----------+---------+----------+--------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in nonulldf.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a3085df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('User_ID', 'int'),\n",
       " ('Product_ID', 'string'),\n",
       " ('City_Category', 'string'),\n",
       " ('Stay_In_Current_City_Years', 'string'),\n",
       " ('Marital_Status', 'int'),\n",
       " ('Product_Category_1', 'int'),\n",
       " ('Product_Category_2', 'int'),\n",
       " ('Product_Category_3', 'int'),\n",
       " ('Purchase', 'int'),\n",
       " ('Gender', 'string'),\n",
       " ('Age', 'string'),\n",
       " ('Occupation', 'int'),\n",
       " ('City_Code', 'string'),\n",
       " ('State_Code', 'string'),\n",
       " ('Zip_Code', 'int'),\n",
       " ('City_Name', 'string'),\n",
       " ('State', 'string')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56a4fa0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Data type StringType is not supported.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o859.transform.\n: java.lang.IllegalArgumentException: Data type StringType is not supported.\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:121)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:117)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:117)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:54)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cbb40996242e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Data type StringType is not supported.'"
     ]
    }
   ],
   "source": [
    "#ML modelling\n",
    "features = ['Product_ID','Marital_Status','Gender','Age','City_Name','State']\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols = features,outputCol='features')\n",
    "new_df = assembler.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574accfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
